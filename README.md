# LangChain  with LLAMA 3.2 Model 

This project demonstrates the usage of LangChain with the LLAMA 3.2 model API (via Ollama) to build an interactive Streamlit app. The app allows users to enter a question, which is then processed by the model and the result is displayed dynamically.

## Table of Contents
- [Project Overview](#project-overview)
- [Requirements](#requirements)
- [Setup](#setup)
- [Usage](#usage)
- [License](#license)

## Project Overview

This project utilizes **LangChain**, a framework designed to facilitate building LLM-powered applications. The app is built using **Streamlit** to provide a web interface, and it integrates the **Ollama API** for accessing the **LLAMA 3.2 model** for natural language processing tasks.

- **LangChain**: A framework to manage and interact with large language models (LLMs).
- **Ollama API**: API to access the LLAMA models.
- **Streamlit**: A framework to build and deploy data science applications, used to create the user interface.
- **dotenv**: A tool to load environment variables from a `.env` file.

The application allows users to input a question, which is then passed to the LLAMA model. The model generates a response, which is displayed to the user.

## Requirements

You will need to install the following Python libraries to run this project:

- `langchain`
- `langchain-ollama`
- `streamlit`
- `python-dotenv`

To install these libraries, you can use the following command:

```bash
pip install langchain langchain-ollama streamlit python-dotenv
```

## Setup

### 1. Clone the repository

First, clone the repository to your local machine:

```bash
git clone https://github.com/your-username/Updated-Langchain.git
cd Updated-Langchain
```

### 2. Install Dependencies

Install the required libraries:

```bash
pip install langchain langchain-ollama streamlit python-dotenv
```

### 3. Set up Environment Variables

This project uses environment variables to store the **LangChain API key** and enables **LangChain tracing**.

- Create a `.env` file in the root directory of your project.
- Add the following content to the `.env` file:

```env
LANGCHAIN_API_KEY=your_api_key_here
```

Replace `your_api_key_here` with your actual LangChain API key. You can get the API key from [LangChain API documentation](https://langchain.com/).

## Usage

To run the Streamlit application, execute the following command:

```bash
streamlit run your_script_name.py
```

Once the app is running, open your browser and go to:

```
http://localhost:8501
```

You can now interact with the app, inputting a question, and getting responses generated by the LLAMA 3.2 model.

### Example:

1. Open the app in your browser.
2. Type a question in the input field.
3. Click Enter, and the app will query the LLAMA model and display the response.

## Code Overview

### 1. **LangChain Setup**

- The script loads environment variables using the `dotenv` library.
- It sets up LangChain with an API key and enables tracing for debugging purposes.

### 2. **Prompt Template**

The `ChatPromptTemplate` is used to define the structure of the conversation between the user and the model. It includes:
- A system message that sets the behavior of the assistant.
- A user message that receives input from the user.

### 3. **Streamlit Framework**

The app uses Streamlit to:
- Display a title on the page.
- Provide an input field for the user to enter a question.
- Output the result from the model when a question is asked.

### 4. **Ollama and LLAMA Model**

The `OllamaLLM` is used to interface with the LLAMA 3.2 model (from Ollama). It sends the prompt to the model, receives the response, and outputs it via LangChainâ€™s `StrOutputParser`.

## License

This project is open-source and available under the [MIT License](LICENSE).
